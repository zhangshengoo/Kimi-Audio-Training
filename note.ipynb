{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Qwen Lora微调实现相关代码\n",
    "Based on the codebase examination, I'll now provide a comprehensive overview of the LoRA implementation in this workspace:\n",
    "\n",
    "# Qwen LoRA Implementation Details\n",
    "\n",
    "## 1. LoRA Configuration\n",
    "\n",
    "The LoRA implementation in this codebase uses the PEFT (Parameter-Efficient Fine-Tuning) library. The core LoRA parameters are defined in `finetune.py`:\n",
    "\n",
    "```python\n",
    "@dataclass\n",
    "class LoraArguments:\n",
    "    lora_r: int = 64                # Rank of the LoRA update matrices\n",
    "    lora_alpha: int = 16            # Scaling factor for the LoRA update\n",
    "    lora_dropout: float = 0.05      # Dropout probability for LoRA layers\n",
    "    lora_target_modules: List[str] = field(\n",
    "        default_factory=lambda: [\"c_attn\", \"c_proj\", \"w1\", \"w2\"]  # Target modules to apply LoRA\n",
    "    )\n",
    "    lora_weight_path: str = \"\"      # Path to pre-trained LoRA weights\n",
    "    lora_bias: str = \"none\"         # Bias configuration\n",
    "    q_lora: bool = False            # Whether to use Quantized LoRA (Q-LoRA)\n",
    "```\n",
    "\n",
    "## 2. LoRA Setup in Training\n",
    "\n",
    "The key LoRA setup happens in the `train()` function in `finetune.py`:\n",
    "\n",
    "```python\n",
    "if training_args.use_lora:\n",
    "    # Decide which modules to save completely (not just the LoRA adapters)\n",
    "    if lora_args.q_lora or is_chat_model:\n",
    "        modules_to_save = None\n",
    "    else:\n",
    "        modules_to_save = [\"wte\", \"lm_head\"]  # For base models, save embedding and output layers\n",
    "    \n",
    "    # Configure LoRA\n",
    "    lora_config = LoraConfig(\n",
    "        r=lora_args.lora_r,\n",
    "        lora_alpha=lora_args.lora_alpha,\n",
    "        target_modules=lora_args.lora_target_modules,\n",
    "        lora_dropout=lora_args.lora_dropout,\n",
    "        bias=lora_args.lora_bias,\n",
    "        task_type=\"CAUSAL_LM\",\n",
    "        modules_to_save=modules_to_save  # Special handling for token embeddings and output layer\n",
    "    )\n",
    "    \n",
    "    # For Q-LoRA, prepare the model for k-bit training\n",
    "    if lora_args.q_lora:\n",
    "        model = prepare_model_for_kbit_training(\n",
    "            model, use_gradient_checkpointing=training_args.gradient_checkpointing\n",
    "        )\n",
    "\n",
    "    # Convert the model to a PEFT model with LoRA adapters\n",
    "    model = get_peft_model(model, lora_config)\n",
    "\n",
    "    # Display trainable parameter information\n",
    "    model.print_trainable_parameters()\n",
    "\n",
    "    # Enable gradient checkpointing if requested\n",
    "    if training_args.gradient_checkpointing:\n",
    "        model.enable_input_require_grads()\n",
    "```\n",
    "\n",
    "## 3. Model Loading for LoRA\n",
    "\n",
    "Before applying LoRA, the model is loaded with appropriate configurations:\n",
    "\n",
    "```python\n",
    "# Load model and tokenizer\n",
    "model = transformers.AutoModelForCausalLM.from_pretrained(\n",
    "    model_args.model_name_or_path,\n",
    "    config=config,\n",
    "    cache_dir=training_args.cache_dir,\n",
    "    device_map=device_map,\n",
    "    trust_remote_code=True,\n",
    "    quantization_config=GPTQConfig(\n",
    "        bits=4, disable_exllama=True\n",
    "    )\n",
    "    if training_args.use_lora and lora_args.q_lora\n",
    "    else None,  # Quantization config for Q-LoRA\n",
    "    **model_load_kwargs,\n",
    ")\n",
    "```\n",
    "\n",
    "## 4. Saving LoRA Weights\n",
    "\n",
    "The code includes a special function to handle saving LoRA weights, particularly with DeepSpeed:\n",
    "\n",
    "```python\n",
    "def safe_save_model_for_hf_trainer(trainer: transformers.Trainer, output_dir: str, bias=\"none\"):\n",
    "    \"\"\"Collects the state dict and dump to disk.\"\"\"\n",
    "    # Check if zero3 mode enabled\n",
    "    if deepspeed.is_deepspeed_zero3_enabled():\n",
    "        state_dict = trainer.model_wrapped._zero3_consolidated_16bit_state_dict()\n",
    "    else:\n",
    "        if trainer.args.use_lora:\n",
    "            # Special handling for LoRA weights\n",
    "            state_dict = get_peft_state_maybe_zero_3(\n",
    "                trainer.model.named_parameters(), bias\n",
    "            )\n",
    "        else:\n",
    "            state_dict = trainer.model.state_dict()\n",
    "    if trainer.args.should_save and trainer.args.local_rank == 0:\n",
    "        trainer._save(output_dir, state_dict=state_dict)\n",
    "```\n",
    "\n",
    "The function `get_peft_state_maybe_zero_3` handles extracting LoRA parameters, especially with DeepSpeed Zero-3 optimization:\n",
    "\n",
    "```python\n",
    "def get_peft_state_maybe_zero_3(named_params, bias):\n",
    "    if bias == \"none\":\n",
    "        to_return = {k: t for k, t in named_params if \"lora_\" in k}\n",
    "    elif bias == \"all\":\n",
    "        to_return = {k: t for k, t in named_params if \"lora_\" in k or \"bias\" in k}\n",
    "    elif bias == \"lora_only\":\n",
    "        # Complex logic to handle LoRA bias parameters\n",
    "        to_return = {}\n",
    "        maybe_lora_bias = {}\n",
    "        lora_bias_names = set()\n",
    "        for k, t in named_params:\n",
    "            if \"lora_\" in k:\n",
    "                to_return[k] = t\n",
    "                bias_name = k.split(\"lora_\")[0] + \"bias\"\n",
    "                lora_bias_names.add(bias_name)\n",
    "            elif \"bias\" in k:\n",
    "                maybe_lora_bias[k] = t\n",
    "        for k, t in maybe_lora_bias:\n",
    "            if bias_name in lora_bias_names:\n",
    "                to_return[bias_name] = t\n",
    "    else:\n",
    "        raise NotImplementedError\n",
    "    \n",
    "    # Handle DeepSpeed Zero-3 parameters\n",
    "    to_return = {k: maybe_zero_3(v) for k, v in to_return.items()}\n",
    "    return to_return\n",
    "```\n",
    "\n",
    "## 5. Running LoRA Fine-tuning\n",
    "\n",
    "### Single GPU LoRA\n",
    "\n",
    "The script `finetune/finetune_lora_single_gpu.sh` contains the configuration for running LoRA fine-tuning on a single GPU:\n",
    "\n",
    "```bash\n",
    "python finetune.py \\\n",
    "  --model_name_or_path $MODEL \\\n",
    "  --data_path $DATA \\\n",
    "  --bf16 True \\\n",
    "  --output_dir output_qwen \\\n",
    "  --num_train_epochs 5 \\\n",
    "  --per_device_train_batch_size 2 \\\n",
    "  --per_device_eval_batch_size 1 \\\n",
    "  --gradient_accumulation_steps 8 \\\n",
    "  --evaluation_strategy \"no\" \\\n",
    "  --save_strategy \"steps\" \\\n",
    "  --save_steps 1000 \\\n",
    "  --save_total_limit 10 \\\n",
    "  --learning_rate 3e-4 \\\n",
    "  --weight_decay 0.1 \\\n",
    "  --adam_beta2 0.95 \\\n",
    "  --warmup_ratio 0.01 \\\n",
    "  --lr_scheduler_type \"cosine\" \\\n",
    "  --logging_steps 1 \\\n",
    "  --report_to \"none\" \\\n",
    "  --model_max_length 512 \\\n",
    "  --lazy_preprocess True \\\n",
    "  --gradient_checkpointing \\\n",
    "  --use_lora\n",
    "```\n",
    "\n",
    "### Distributed LoRA Training\n",
    "\n",
    "For multi-GPU training, the script `finetune/finetune_lora_ds.sh` uses DeepSpeed with ZeRO-2 optimization:\n",
    "\n",
    "```bash\n",
    "torchrun $DISTRIBUTED_ARGS finetune.py \\\n",
    "    --model_name_or_path $MODEL \\\n",
    "    --data_path $DATA \\\n",
    "    --bf16 True \\\n",
    "    --output_dir output_qwen \\\n",
    "    # ... (same parameters as single GPU) ...\n",
    "    --use_lora \\\n",
    "    --gradient_checkpointing \\\n",
    "    --deepspeed ${DS_CONFIG_PATH}\n",
    "```\n",
    "\n",
    "## 6. Quantized LoRA (Q-LoRA)\n",
    "\n",
    "The codebase also supports Q-LoRA, which uses a 4-bit quantized model with LoRA for more memory-efficient fine-tuning:\n",
    "\n",
    "```python\n",
    "# In finetune.py\n",
    "quantization_config=GPTQConfig(\n",
    "    bits=4, disable_exllama=True\n",
    ")\n",
    "if training_args.use_lora and lora_args.q_lora\n",
    "else None,\n",
    "```\n",
    "\n",
    "The Q-LoRA script `finetune/finetune_qlora_single_gpu.sh` uses:\n",
    "\n",
    "```bash\n",
    "python finetune.py \\\n",
    "  --model_name_or_path $MODEL \\\n",
    "  --data_path $DATA \\\n",
    "  --fp16 True \\  # Note: fp16 instead of bf16 for quantized models\n",
    "  # ... (same parameters as regular LoRA) ...\n",
    "  --use_lora \\\n",
    "  --q_lora \\  # Enable Q-LoRA\n",
    "  --deepspeed finetune/ds_config_zero2.json\n",
    "```\n",
    "\n",
    "## 7. Special Handling for Base vs. Chat Models\n",
    "\n",
    "The code has special handling for base language models vs. chat models:\n",
    "\n",
    "```python\n",
    "is_chat_model = 'chat' in model_args.model_name_or_path.lower()\n",
    "if training_args.use_lora:\n",
    "    if lora_args.q_lora or is_chat_model:\n",
    "        modules_to_save = None\n",
    "    else:\n",
    "        modules_to_save = [\"wte\", \"lm_head\"]  # Save token embedding and output layers for base models\n",
    "```\n",
    "\n",
    "This is because base models need to learn the special tokens used in the chat format, so the embedding and output layers need to be trained as well.\n",
    "\n",
    "## 8. DeepSpeed Configuration\n",
    "\n",
    "LoRA fine-tuning uses ZeRO-2 optimization as defined in `finetune/ds_config_zero2.json`:\n",
    "\n",
    "```json\n",
    "\"zero_optimization\": {\n",
    "    \"stage\": 2,\n",
    "    \"offload_optimizer\": {\n",
    "        \"device\": \"none\",\n",
    "        \"pin_memory\": true\n",
    "    },\n",
    "    \"allgather_partitions\": true,\n",
    "    \"allgather_bucket_size\": 2e8,\n",
    "    \"overlap_comm\": true,\n",
    "    \"reduce_scatter\": true,\n",
    "    \"reduce_bucket_size\": 2e8,\n",
    "    \"contiguous_gradients\": true\n",
    "}\n",
    "```\n",
    "\n",
    "This optimizes memory usage while allowing efficient distributed training of the LoRA adapters."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "实测下来，Kimi-Audio-7B-Instruct load_detokenizer=False\n",
    "显存占用位 23420 MB\n",
    "ALM: 9766.33 M\n",
    "Whisper: 636.97 M"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
